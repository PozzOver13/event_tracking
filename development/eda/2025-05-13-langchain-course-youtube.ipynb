{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🔍 **LangChain: Overview della libreria**\n",
    "\n",
    "🎯 **Perché LangChain?**\n",
    "\n",
    "LangChain, sia come **libreria Python** che come **azienda**, ha come obiettivo principale quello di **semplificare lo sviluppo di applicazioni che ragionano** utilizzando modelli linguistici di grandi dimensioni (LLM).\n",
    "\n",
    "💡 **Problemi chiave affrontati da LangChain**\n",
    "\n",
    "LangChain si propone di risolvere tre bisogni fondamentali nello sviluppo di applicazioni AI basate su LLM:\n",
    "\n",
    "1. **Interfacce standardizzate** per componenti riutilizzabili (prompt, LLM, memory, strumenti, agenti).\n",
    "2. **Orchestrazione** delle componenti per creare pipeline complesse.\n",
    "3. **Osservabilità e valutazione**, per monitorare e migliorare il comportamento dei modelli in produzione.\n",
    "\n",
    "\n",
    "🧠 **Architettura Cognitiva**\n",
    "\n",
    "LangChain adotta una **struttura modulare** in cui ogni componente dell'applicazione contribuisce a una sorta di \"architettura cognitiva\" del sistema. Queste componenti includono:\n",
    "\n",
    "* **Chains**: sequenze di chiamate a LLM o strumenti.\n",
    "* **Agents**: modelli che decidono dinamicamente quali azioni compiere.\n",
    "* **Retrievers**: moduli per il recupero di informazioni da fonti esterne.\n",
    "\n",
    "> Questi elementi **non sono legati a integrazioni specifiche**, ma sono **generici e riutilizzabili** in qualunque contesto.\n",
    "\n",
    "\n",
    "🔌 **Integrazioni**\n",
    "\n",
    "LangChain distingue tra il core della libreria e le integrazioni esterne. Le **integrazioni con modelli e servizi terzi** sono gestite in **pacchetti separati**, ad esempio:\n",
    "\n",
    "* `langchain-openai`\n",
    "* `langchain-anthropic`\n",
    "* `langchain-mistral`\n",
    "  Questa modularità **migliora la leggerezza** e la **gestione delle versioni**.\n",
    "\n",
    "\n",
    "🚀 **Funzionalità avanzate con i modelli di nuova generazione**\n",
    "\n",
    "LangChain sfrutta appieno le **API avanzate dei modelli recenti**, abilitando nuovi casi d'uso:\n",
    "\n",
    "* **Tool Calling**\n",
    "  Permette ai modelli di interagire direttamente con strumenti esterni (API, database, servizi), arricchendo il comportamento dell'agente.\n",
    "\n",
    "* **Structured Output**\n",
    "  Fa sì che il modello restituisca risposte in formati strutturati (es. JSON validati su schema), facilitando l'integrazione con pipeline software.\n",
    "\n",
    "* **Multimodalità**\n",
    "  Supporta input e output multimodali (immagini, audio, video) oltre al solo testo.\n",
    "\n",
    "\n",
    "📚 Risorse principali\n",
    "\n",
    "* Documentazione ufficiale: [LangChain Docs](https://python.langchain.com/docs/concepts/)"
   ],
   "id": "2c9a8092eec91ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangChain Expression Language\n",
    "\n",
    "Il **LangChain Expression Language (LCEL)** è una **nuova sintassi dichiarativa** introdotta da LangChain per **comporre pipeline di LLM** (modelli di linguaggio) in modo semplice, leggibile e modulare.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 In breve: cos'è LCEL?\n",
    "\n",
    "È un **mini-linguaggio interno a Python** che ti permette di costruire catene (`chains`) di operazioni LLM **come fossero funzioni componibili**, usando una sintassi **simile a quella delle espressioni matematiche o dei DSL (Domain-Specific Language)**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Obiettivo\n",
    "\n",
    "LCEL nasce per:\n",
    "\n",
    "* Facilitare la **composizione di più step** (prompting, parsing, tool calling, ecc.).\n",
    "* Ridurre il **boilerplate code**.\n",
    "* Rendere le catene **più leggibili e testabili**.\n",
    "* Rendere LangChain più **declarativo** (come una DAG) rispetto all’approccio puramente imperativo (`SequentialChain`, `LLMChain`, ecc.).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 Esempio base (senza LCEL)\n",
    "\n",
    "Tradizionalmente, avresti scritto:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = PromptTemplate.from_template(\"Come si dice '{word}' in inglese?\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "result = chain.run(\"ciao\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✨ Stesso esempio con LCEL\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = PromptTemplate.from_template(\"Come si dice '{word}' in inglese?\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"word\": \"ciao\"})\n",
    "```\n",
    "\n",
    "> Noti la **pipeline** `|` che unisce i pezzi: `prompt` → `llm`\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Composizione modulare\n",
    "\n",
    "Puoi aggiungere facilmente altre parti alla catena:\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "```\n",
    "\n",
    "Oppure combinare più input/output usando `RunnableMap` o `RunnableLambda`.\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 Tipi di componenti che puoi combinare in LCEL\n",
    "\n",
    "* `PromptTemplate`\n",
    "* `LLM` (es. `ChatOpenAI`)\n",
    "* `OutputParser`\n",
    "* `RunnableLambda` (funzioni custom)\n",
    "* `RunnableMap` (parallelizzazione)\n",
    "* `RunnableSequence` (catene complesse)\n",
    "* `Retrieval`, `ToolCall`, `FunctionCall`\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Esempio più avanzato\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.output_parsers import JsonOutputParser\n",
    "\n",
    "def enrich_input(x):\n",
    "    return {\"word\": x.upper()}\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "chain = (\n",
    "    RunnableLambda(enrich_input) |\n",
    "    prompt |\n",
    "    llm |\n",
    "    parser\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"ciao\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Vantaggi di LCEL\n",
    "\n",
    "| Vantaggio               | Descrizione                                  |                 |\n",
    "| ----------------------- | -------------------------------------------- | --------------- |\n",
    "| ✅ Leggibile             | Codice più chiaro e lineare                  |                 |\n",
    "| ✅ Componibile           | Puoi unire moduli con \\`                     | \\` come in Unix |\n",
    "| ✅ Testing facile        | Ogni blocco può essere testato separatamente |                 |\n",
    "| ✅ Supporta streaming    | Supporto nativo a `.stream()`                |                 |\n",
    "| ✅ Asincrono e parallelo | Con `.ainvoke()`, `.batch()`, ecc.           |                 |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔚 Conclusione\n",
    "\n",
    "Il **LangChain Expression Language (LCEL)** è un modo **moderno e modulare** per costruire catene LLM con LangChain, che sostituisce (o affianca) l'approccio più tradizionale con `LLMChain` e `SequentialChain`.\n",
    "\n"
   ],
   "id": "676b6470a3e2118c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Youtube Course \"LangChain Mastery in 2025 | Full 5 Hour Course [LangChain v0.3]\"",
   "id": "8390110f61d1a254"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Intro\n",
    "\n",
    "https://github.com/aurelio-labs/langchain-course/blob/main/chapters/01-intro.ipynb"
   ],
   "id": "f3a24319b1b07852"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T06:38:48.058533Z",
     "start_time": "2025-05-20T06:38:44.954070Z"
    }
   },
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "#\n",
    "# llm = Ollama(model=\"gemma2:2b\", temperature=0)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "#openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:46:23.400638Z",
     "start_time": "2025-05-16T07:46:23.394169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "article = \"\"\"\n",
    "\\\n",
    "We believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\n",
    "\n",
    "With agents, we allow LLMs to integrate with code — allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\n",
    "\n",
    "In this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\n",
    "\n",
    "---\n",
    "\n",
    "## Neuro-Symbolic Systems\n",
    "\n",
    "Neuro-symbolic systems consist of both neural and symbolic computation, where:\n",
    "\n",
    "- Neural refers to LLMs, embedding models, or other neural network-based models.\n",
    "- Symbolic refers to logic containing symbolic logic, such as code.\n",
    "\n",
    "Both neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions.\n",
    "\n",
    "The other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt's perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter.\n",
    "\n",
    "Most important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\n",
    "\n",
    "| Neural | Symbolic |\n",
    "| --- | --- |\n",
    "| Flexible, learned logic that can cover a huge range of potential scenarios. | Mostly hand-written rules which can be very granular and fine-tuned but hard to scale. |\n",
    "| Hard to interpret why a neural system does what it does. Very difficult or even impossible to predict behavior. | Rules are written and can be understood. When unsure why a particular ouput was produced we can look at the rules / logic to understand. |\n",
    "| Requires huge amount of data and compute to train state-of-the-art neural models, making it hard to add new abilities or update with new information. | Code is relatively cheap to write, it can be updated with new features easily, and latest information can often be added often instantaneously. |\n",
    "| When trained on broad datasets can often lack performance when exposed to unique scenarios that are not well represented in the training data. | Easily customized to unique scenarios. |\n",
    "| Struggles with complex computations such as mathematical operations. | Perform complex computations very quickly and accurately. |\n",
    "\n",
    "Pure neural architectures struggle with many seemingly simple tasks. For example, an LLM *cannot* provide an accurate answer if we ask it for today's date.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is commonly used to provide LLMs with up-to-date knowledge on a particular subject or access to proprietary knowledge.\n",
    "\n",
    "### Giving LLMs Superpowers\n",
    "\n",
    "By 2020, it was becoming clear that neural AI systems could not perform tasks symbolic systems typically excelled in, such as arithmetic, accessing structured DB data, or making API calls. These tasks require discrete input parameters that allow us to process them reliably according to strict written logic.\n",
    "\n",
    "In 2022, researchers at AI21 developed Jurassic-X, an LLM-based \"neuro-symbolic architecture.\" Neuro-symbolic refers to merging the \"neural computation\" of large language models (LLMs) with more traditional (i.e. symbolic) computation of code.\n",
    "\n",
    "Jurassic-X used the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. The researchers developed MRKL to solve the limitations of LLMs, namely:\n",
    "\n",
    "- Lack of up-to-date knowledge, whether that is the latest in AI or something as simple as today's date.\n",
    "- Lack of proprietary knowledge, such as internal company docs or your calendar bookings.\n",
    "- Lack of reasoning, i.e. the inability to perform operations that traditional software is good at, like running complex mathematical operations.\n",
    "- Lack of ability to generalize. Back in 2022, most LLMs had to be fine-tuned to perform well in a specific domain. This problem is still present today but far less prominent as the SotA models generalize much better and, in the case of MRKL, are able to use tools relatively well (although we could certainly take the MRKL solution to improve tool use performance even today).\n",
    "\n",
    "MRKL represents one of the earliest forms of what we would now call an agent; it is an LLM (neural computation) paired with executable code (symbolic computation).\n",
    "\n",
    "## ReAct and Tools\n",
    "\n",
    "There is a misconception in the broader industry that an AI agent is an LLM contained within some looping logic that can generate inputs for and execute code functions. This definition of agents originates from the huge popularity of the ReAct agent framework and the adoption of a similar structure with function/tool calling by LLM providers such as OpenAI, Anthropic, and Ollama.\n",
    "\n",
    "![ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.](/images/posts/ai-agents/ai-agents-00.png)\n",
    "\n",
    "<small>ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.</small>\n",
    "\n",
    "Our \"neuro-symbolic\" definition is much broader but certainly does include ReAct agents and LLMs paired with tools. This agent type is the most common for now, so it's worth understanding the basic concept behind it.\n",
    "\n",
    "The **Re**ason **Act**ion (ReAct) method encourages LLMs to generate iterative *reasoning* and *action* steps. During *reasoning,* the LLM describes what steps are to be taken to answer the user's query. Then, the LLM generates an *action,* which we parse into an input to some executable code, which we typically describe as a tool/function call.\n",
    "\n",
    "![ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.](/images/posts/ai-agents/ai-agents-01.png)\n",
    "\n",
    "<small>ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.</small>\n",
    "\n",
    "Following the reason and action steps, our action tool call returns an observation. The logic returns the observation to the LLM, which is then used to generate subsequent reasoning and action steps.\n",
    "\n",
    "The ReAct loop continues until the LLM has enough information to answer the original input. Once the LLM reaches this state, it calls a special *answer* action with the generated answer for the user.\n",
    "\n",
    "## Not only LLMs and Tool Calls\n",
    "\n",
    "LLMs paired with tool calling are powerful but far from the only approach to building agents. Using the definition of neuro-symbolic, we cover architectures such as:\n",
    "\n",
    "- Multi-agent workflows that involve multiple LLM-tool (or other agent structure) combinations.\n",
    "- More deterministic workflows where we may have set neural model-tool paths that may fork or merge as the use case requires.\n",
    "- Embedding models that can detect user intents and decide tool-use or LLM selection-based selection in vector space.\n",
    "\n",
    "These are just a few high-level examples of alternative agent structures. Far from being designed for niche use cases, we find these alternative options to frequently perform better than the more common ReAct or Tool agents. We will cover all of these examples and more in future chapters.\n",
    "\n",
    "---\n",
    "\n",
    "Agents are fundamental to the future of AI, but that doesn't mean we should expect that future to come from agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader, and we believe thinking beyond ReAct and Tools is key to building future AI.\n",
    "\n",
    "---\n",
    "\n",
    "You can sign up for the [Aurelio AI newsletter](https://b0fcw9ec53w.typeform.com/to/w2BDHVK7) to stay updated on future releases in our comprehensive course on agents.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1] The curious case of Connectionism (2019) [https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html](https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html)\n",
    "\n",
    "[2] F. Rosenblatt, [The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) (1958), Psychological Review\n",
    "\n",
    "[3] E. Karpas et al. [MRKL Systems: A Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning](https://arxiv.org/abs/2205.00445) (2022), AI21 Labs\n",
    "\"\"\""
   ],
   "id": "24432c3371e854a5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## First Prompt",
   "id": "2a705b61a585446d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:46:31.824950Z",
     "start_time": "2025-05-16T07:46:31.723798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "# Defining the system prompt (how the AI should act)\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant called {name} that helps generate article titles.\",\n",
    "    input_variables=[\"name\"]\n",
    ")\n",
    "\n",
    "# the user prompt is provided by the user, in this case however the only dynamic\n",
    "# input is the article\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a name for a article.\n",
    "The article is here for you to examine\n",
    "\n",
    "---\n",
    "{article}\n",
    "---\n",
    "\n",
    "The name should be based of the context of the article.\n",
    "Be creative, but make sure the names are clear, catchy,\n",
    "and relevant to the theme of the article.\n",
    "\n",
    "Only output the article name, no other explanation or\n",
    "text can be provided.\"\"\",\n",
    "    input_variables=[\"article\"]\n",
    ")"
   ],
   "id": "74f836198dae2e20",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:46:33.262556Z",
     "start_time": "2025-05-16T07:46:33.255807Z"
    }
   },
   "cell_type": "code",
   "source": "user_prompt.format(article=\"TEST STRING\")",
   "id": "3b2545ea01ba0a82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='You are tasked with creating a name for a article.\\nThe article is here for you to examine\\n\\n---\\nTEST STRING\\n---\\n\\nThe name should be based of the context of the article.\\nBe creative, but make sure the names are clear, catchy,\\nand relevant to the theme of the article.\\n\\nOnly output the article name, no other explanation or\\ntext can be provided.', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:46:34.622423Z",
     "start_time": "2025-05-16T07:46:34.618316Z"
    }
   },
   "cell_type": "code",
   "source": "print(user_prompt.format(article=\"TEST STRING\").content)",
   "id": "80272c307d29aee9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are tasked with creating a name for a article.\n",
      "The article is here for you to examine\n",
      "\n",
      "---\n",
      "TEST STRING\n",
      "---\n",
      "\n",
      "The name should be based of the context of the article.\n",
      "Be creative, but make sure the names are clear, catchy,\n",
      "and relevant to the theme of the article.\n",
      "\n",
      "Only output the article name, no other explanation or\n",
      "text can be provided.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:46:37.693995Z",
     "start_time": "2025-05-16T07:46:37.690587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ],
   "id": "fe5350d598b245ff",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:46:38.601782Z",
     "start_time": "2025-05-16T07:46:38.597386Z"
    }
   },
   "cell_type": "code",
   "source": "print(first_prompt.format(name=\"JARVIS\", article=\"TEST STRING\"))",
   "id": "ee083f86239be509",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an AI assistant called JARVIS that helps generate article titles.\n",
      "Human: You are tasked with creating a name for a article.\n",
      "The article is here for you to examine\n",
      "\n",
      "---\n",
      "TEST STRING\n",
      "---\n",
      "\n",
      "The name should be based of the context of the article.\n",
      "Be creative, but make sure the names are clear, catchy,\n",
      "and relevant to the theme of the article.\n",
      "\n",
      "Only output the article name, no other explanation or\n",
      "text can be provided.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:46:43.180258Z",
     "start_time": "2025-05-16T07:46:43.176566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain_one = (\n",
    "        {\n",
    "            \"article\": lambda x: x[\"article\"],\n",
    "            \"name\": lambda x: x[\"name\"]\n",
    "        }\n",
    "        | first_prompt\n",
    "        | llm\n",
    "        | {\"article_title\": lambda x: x.content}\n",
    ")"
   ],
   "id": "50c1506a3926ff12",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:46:46.369629Z",
     "start_time": "2025-05-16T07:46:45.049225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_chain = {\"article\": article,\n",
    "               \"name\": \"JARVIS\"}\n",
    "article_title_msg = chain_one.invoke(input_chain)\n",
    "article_title_msg"
   ],
   "id": "a1a0239f3a956dc3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_title': '\"Unveiling the Future of AI: The Rise of Neuro-Symbolic Agents\"'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Second Prompt",
   "id": "edb88303b63ae57d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:46:50.697300Z",
     "start_time": "2025-05-16T07:46:50.694251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a description for\n",
    "the article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Here is the article title '{article_title}'.\n",
    "\n",
    "Output the SEO friendly article description. Do not output\n",
    "anything other than the description.\"\"\",\n",
    "    input_variables=[\"article\", \"article_title\"]\n",
    ")\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    second_user_prompt\n",
    "])"
   ],
   "id": "afa8a6c2c856cd0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:46:54.943297Z",
     "start_time": "2025-05-16T07:46:54.939005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain_two = (\n",
    "        {\n",
    "            \"name\": lambda x: x[\"name\"],\n",
    "            \"article\": lambda x: x[\"article\"],\n",
    "            \"article_title\": lambda x: x[\"article_title\"]\n",
    "        }\n",
    "        | second_prompt\n",
    "        | llm\n",
    "        | {\"summary\": lambda x: x.content}\n",
    ")"
   ],
   "id": "4ec498d67ddf646a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:47:01.818415Z",
     "start_time": "2025-05-16T07:47:00.792567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "article_description_msg = chain_two.invoke({\n",
    "    \"name\": \"JARVIS\",\n",
    "    \"article\": article,\n",
    "    \"article_title\": article_title_msg[\"article_title\"]\n",
    "})\n",
    "article_description_msg"
   ],
   "id": "766f125863828738",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Discover how neuro-symbolic agents are shaping the future of AI, merging neural AI with traditional software to unlock a wide range of capabilities and applications. Explore the essential concepts behind AI agents and their pivotal role in real-world AI advancements.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Third Prompt",
   "id": "56c7f8a1c85cce59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:47:08.434269Z",
     "start_time": "2025-05-16T07:47:08.430563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "third_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a new paragraph for the\n",
    "article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Choose one paragraph to review and edit. During your edit\n",
    "ensure you provide constructive feedback to the user so they\n",
    "can learn where to improve their own writing.\"\"\",\n",
    "    input_variables=[\"article\"]\n",
    ")\n",
    "\n",
    "# prompt template 3: creating a new paragraph for the article\n",
    "third_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    third_user_prompt\n",
    "])"
   ],
   "id": "8b197787cbc2151c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:47:11.972417Z",
     "start_time": "2025-05-16T07:47:11.937117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Paragraph(BaseModel):\n",
    "    original_paragraph: str = Field(description=\"The original paragraph\")\n",
    "    edited_paragraph: str = Field(description=\"The improved edited paragraph\")\n",
    "    feedback: str = Field(description=(\n",
    "        \"Constructive feedback on the original paragraph\"\n",
    "    ))\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Paragraph)"
   ],
   "id": "81f67aaad8a039a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cg08900\\Documents\\Pandora\\github\\event_tracking\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1673: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:48:39.840642Z",
     "start_time": "2025-05-16T07:48:39.836803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# chain 3: inputs: article / output: article_para\n",
    "chain_three = (\n",
    "        {\"name\": lambda x: x[\"name\"],\n",
    "         \"article\": lambda x: x[\"article\"]}\n",
    "        | third_prompt\n",
    "        | structured_llm\n",
    "        | {\n",
    "            \"original_paragraph\": lambda x: x.original_paragraph,\n",
    "            \"edited_paragraph\": lambda x: x.edited_paragraph,\n",
    "            \"feedback\": lambda x: x.feedback\n",
    "        }\n",
    ")"
   ],
   "id": "69cc4d22ca3c9168",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T07:48:43.806983Z",
     "start_time": "2025-05-16T07:48:41.943094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = chain_three.invoke(input_chain)\n",
    "out"
   ],
   "id": "5be691dba1785c40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_paragraph': \"Agents are fundamental to the future of AI, but that doesn't mean we should expect that future to come from agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader, and we believe thinking beyond ReAct and Tools is key to building future AI.\",\n",
       " 'edited_paragraph': 'Agents play a crucial role in shaping the future of AI. While ReAct and Tool agents excel in handling numerous simple use cases effectively, it is essential to recognize that the potential of agents extends far beyond these conventional forms. Embracing a broader perspective and exploring innovative approaches beyond ReAct and Tools is vital for advancing the field of AI.',\n",
       " 'feedback': 'The original paragraph effectively conveys the importance of agents in the future of AI and highlights the limitations of relying solely on ReAct and Tool agents. The edited paragraph enhances the clarity and impact of the message by emphasizing the need to think beyond traditional agent structures. It effectively communicates the idea that exploring diverse approaches is essential for the advancement of AI technology.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:02:48.050799Z",
     "start_time": "2025-05-16T08:02:48.044602Z"
    }
   },
   "cell_type": "code",
   "source": "out['edited_paragraph']",
   "id": "c1e662b091efdc44",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agents play a crucial role in shaping the future of AI. While ReAct and Tool agents excel in handling numerous simple use cases effectively, it is essential to recognize that the potential of agents extends far beyond these conventional forms. Embracing a broader perspective and exploring innovative approaches beyond ReAct and Tools is vital for advancing the field of AI.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Langsmith\n",
    "\n",
    "LangSmith è la piattaforma di osservabilità e valutazione sviluppata da LangChain. Serve per monitorare, debuggare e valutare le applicazioni che usano LLM in produzione.\n",
    "\n",
    "In sintesi, LangSmith offre:\n",
    "🧪 Tracciamento delle esecuzioni di chain e agenti, con logging dettagliato di prompt, risposte, tool e tempi.\n",
    "\n",
    "📊 Valutazione automatica o manuale delle risposte del modello (es. accuratezza, qualità, ecc).\n",
    "\n",
    "🔍 Debug visuale per ispezionare ogni passaggio delle pipeline LLM.\n",
    "\n",
    "📁 Dataset management per testare modelli su casi reali e versionare i prompt.\n",
    "\n",
    "È pensato per aiutare a migliorare qualità e affidabilità delle applicazioni LLM-driven, facilitando il ciclo di sviluppo continuo."
   ],
   "id": "ed6d00373b41f2f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prompts",
   "id": "89f53a228ff70d84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:38:53.044271Z",
     "start_time": "2025-05-20T06:38:53.041036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\"\"\""
   ],
   "id": "30f7236758fdfb90",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:39:28.207528Z",
     "start_time": "2025-05-20T06:39:28.105845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# passing the template to the LangChain model\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ],
   "id": "ada4dccd5b919659",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:40:20.545772Z",
     "start_time": "2025-05-20T06:40:20.539471Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_template.input_variables",
   "id": "cc887a9672202b9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:40:58.584064Z",
     "start_time": "2025-05-20T06:40:58.579486Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_template.messages",
   "id": "aa13296580d3dd6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:42:51.303047Z",
     "start_time": "2025-05-20T06:42:51.299193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# stessa cosa forse meno leggibile ma piu' esplicito\n",
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ],
   "id": "c5db6583cc1fe01f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:43:50.616964Z",
     "start_time": "2025-05-20T06:43:50.612242Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_template.messages",
   "id": "7cb83e3ee81f1370",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:46:54.929095Z",
     "start_time": "2025-05-20T06:46:54.925751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"context\": lambda x: x[\"context\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ],
   "id": "ade5bad78f3447ad",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:46:40.408452Z",
     "start_time": "2025-05-20T06:46:40.403527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Experts in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\""
   ],
   "id": "96edb9f554708971",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:46:59.219304Z",
     "start_time": "2025-05-20T06:46:57.810104Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline.invoke({\"query\": query, \"context\": context})\n",
   "id": "f7a8e08834fe8d3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Aurelio AI is an AI company that focuses on developing tooling for AI engineers. They specialize in language AI and have expertise in building AI agents and information retrieval. The company is known for creating open source frameworks like Semantic Router and Semantic Chunkers, as well as providing an AI Platform for engineers to build with AI. Additionally, they offer development services to help other organizations bring their AI technology to market.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 188, 'total_tokens': 270, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZB2YOa0c37PoGAjCnpUBuhPOju0A', 'finish_reason': 'stop', 'logprobs': None}, id='run-192f5c96-761a-4693-865b-e248788b2c27-0', usage_metadata={'input_tokens': 188, 'output_tokens': 82, 'total_tokens': 270, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Few Shot",
   "id": "18c9184d48f24967"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:51:20.656395Z",
     "start_time": "2025-05-20T06:51:20.651810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])"
   ],
   "id": "2b170b234cfda99c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:51:21.329792Z",
     "start_time": "2025-05-20T06:51:21.326453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "examples = [\n",
    "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
    "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
    "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"},\n",
    "]"
   ],
   "id": "a20791155bbe5e13",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:51:28.846328Z",
     "start_time": "2025-05-20T06:51:28.839618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "# here is the formatted prompt\n",
    "print(few_shot_prompt.format())"
   ],
   "id": "2943e3a56da80b1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Here is query #1\n",
      "AI: Here is the answer #1\n",
      "Human: Here is query #2\n",
      "AI: Here is the answer #2\n",
      "Human: Here is query #3\n",
      "AI: Here is the answer #3\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:53:39.982722Z",
     "start_time": "2025-05-20T06:53:38.553535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Always answer in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet\n",
    "points, then conclude.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template.messages[0].prompt.template = new_system_prompt\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "print(out)"
   ],
   "id": "57d7ace344c4a458",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Aurelio AI's Activities\n",
      "\n",
      "Aurelio AI is involved in various activities related to AI development and services:\n",
      "\n",
      "- **Open Source Frameworks:**\n",
      "  - Developed Semantic Router and Semantic Chunkers.\n",
      "- **AI Platform:**\n",
      "  - Provides tooling for AI engineers to facilitate AI development.\n",
      "- **Development Services:**\n",
      "  - Assists other organizations in bringing their AI technology to market.\n",
      "- **LangChain Experts:**\n",
      "  - Became LangChain Experts in September 2024, showcasing expertise in delivering AI solutions using the LangChain ecosystem. \n",
      "\n",
      "In summary, Aurelio AI focuses on developing AI tools, offering an AI platform, providing development services, and excelling in AI solutions with the LangChain ecosystem.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:53:52.245853Z",
     "start_time": "2025-05-20T06:53:52.240379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out))"
   ],
   "id": "9dbb8d71c3e00fde",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Aurelio AI's Activities\n\nAurelio AI is involved in various activities related to AI development and services:\n\n- **Open Source Frameworks:**\n  - Developed Semantic Router and Semantic Chunkers.\n- **AI Platform:**\n  - Provides tooling for AI engineers to facilitate AI development.\n- **Development Services:**\n  - Assists other organizations in bringing their AI technology to market.\n- **LangChain Experts:**\n  - Became LangChain Experts in September 2024, showcasing expertise in delivering AI solutions using the LangChain ecosystem. \n\nIn summary, Aurelio AI focuses on developing AI tools, offering an AI platform, providing development services, and excelling in AI solutions with the LangChain ecosystem."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:55:33.451151Z",
     "start_time": "2025-05-20T06:55:33.446997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Can you explain gravity?\",\n",
    "        \"output\": (\n",
    "            \"## Gravity\\n\\n\"\n",
    "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
    "            \"### Discovery\\n\\n\"\n",
    "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
    "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
    "            \"### In General Relativity\\n\\n\"\n",
    "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
    "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
    "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
    "            \"### Gravitons\\n\\n\"\n",
    "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
    "            \"* They have not yet been detected.\\n\\n\"\n",
    "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": (\n",
    "            \"## France\\n\\n\"\n",
    "            \"The capital of France is Paris.\\n\\n\"\n",
    "            \"### Origins\\n\\n\"\n",
    "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
    "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
    "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
    "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
    "        )\n",
    "    }\n",
    "]"
   ],
   "id": "134bd1a49646fdd0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:55:42.924169Z",
     "start_time": "2025-05-20T06:55:42.920446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")"
   ],
   "id": "f4382b6b02527aa4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:55:52.691524Z",
     "start_time": "2025-05-20T06:55:52.686281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = few_shot_prompt.format()\n",
    "\n",
    "display(Markdown(out))"
   ],
   "id": "75b5adbd062510cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Human: Can you explain gravity?\nAI: ## Gravity\n\nGravity is one of the fundamental forces in the universe.\n\n### Discovery\n\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n\n### In General Relativity\n\n* Gravity is described as the curvature of spacetime.\n* The more massive an object is, the more it curves spacetime.\n* This curvature is what causes objects to fall towards each other.\n\n### Gravitons\n\n* Gravitons are hypothetical particles that mediate the force of gravity.\n* They have not yet been detected.\n\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n\n\nHuman: What is the capital of France?\nAI: ## France\n\nThe capital of France is Paris.\n\n### Origins\n\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\n\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:57:10.204513Z",
     "start_time": "2025-05-20T06:57:10.199767Z"
    }
   },
   "cell_type": "code",
   "source": "few_shot_prompt",
   "id": "dc0363523558c80a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': 'Can you explain gravity?', 'output': '## Gravity\\n\\nGravity is one of the fundamental forces in the universe.\\n\\n### Discovery\\n\\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n### In General Relativity\\n\\n* Gravity is described as the curvature of spacetime.\\n* The more massive an object is, the more it curves spacetime.\\n* This curvature is what causes objects to fall towards each other.\\n\\n### Gravitons\\n\\n* Gravitons are hypothetical particles that mediate the force of gravity.\\n* They have not yet been detected.\\n\\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n'}, {'input': 'What is the capital of France?', 'output': '## France\\n\\nThe capital of France is Paris.\\n\\n### Origins\\n\\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world\\'s greatest cultural and economic centres.\\n\\n'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:57:16.539006Z",
     "start_time": "2025-05-20T06:57:16.535116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", new_system_prompt),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ],
   "id": "b57919f70fbfc910",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T06:57:23.181775Z",
     "start_time": "2025-05-20T06:57:21.795359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline = prompt_template | llm\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "display(Markdown(out))"
   ],
   "id": "59f7eb93fb5d844",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "## Aurelio AI\n\nAurelio AI is an AI company specializing in language AI tools and services.\n\n### Focus Areas\n\n* **Tooling for AI Engineers**: Developing tools for AI engineers, with a focus on language AI.\n* **Open Source Frameworks**: Behind open source frameworks like Semantic Router and Semantic Chunkers.\n* **AI Platform**: Providing engineers with tooling to assist in building AI solutions.\n* **Development Services**: Offering development services to organizations to help bring their AI tech to market.\n\n### LangChain Experts\n\n* Became LangChain Experts in September 2024.\n* Known for delivering AI solutions built with the LangChain ecosystem.\n\n**In summary**, Aurelio AI is a company deeply involved in the development and deployment of AI tools and services, particularly in the realm of language AI."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chain of Thoughts",
   "id": "5fb9a0bdf783f763"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:00:16.464233Z",
     "start_time": "2025-05-20T07:00:16.459568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "no_cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "You MUST answer the question directly without any other\n",
    "text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", no_cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ],
   "id": "50f240782fd0fe50",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:01:43.999880Z",
     "start_time": "2025-05-20T07:01:43.347320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = (\n",
    "    \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
    ")\n",
    "\n",
    "no_cot_pipeline = no_cot_prompt_template | llm\n",
    "no_cot_result = no_cot_pipeline.invoke({\"query\": query}).content\n",
    "print(no_cot_result) # REAL ANSWER = 1392!!!"
   ],
   "id": "dc27bc423442c6c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of keystrokes needed to type the numbers from 1 to 500 is 1905.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:02:41.612081Z",
     "start_time": "2025-05-20T07:02:41.608047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question, you must:\n",
    "\n",
    "- List systematically and in precise detail all\n",
    "  subproblems that need to be solved to answer the\n",
    "  question.\n",
    "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
    "- Finally, use everything you have worked through to\n",
    "  provide the final answer.\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "cot_pipeline = cot_prompt_template | llm"
   ],
   "id": "fb1f3a17c8d9c7d9",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:03:49.567443Z",
     "start_time": "2025-05-20T07:03:46.035020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cot_result = cot_pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(cot_result))"
   ],
   "id": "ab515456ebc0c481",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "To determine the total number of keystrokes needed to type the numbers from 1 to 500, we can break down the problem into smaller subproblems:\n\n1. Determine the number of keystrokes needed to type each digit from 1 to 9.\n2. Determine the number of keystrokes needed to type each two-digit number from 10 to 99.\n3. Determine the number of keystrokes needed to type each three-digit number from 100 to 500.\n4. Sum up the total number of keystrokes from the above subproblems to get the final answer.\n\nLet's solve each subproblem step by step:\n\n1. Number of keystrokes needed to type each digit from 1 to 9:\n   - Each digit requires one keystroke to type.\n   - Total keystrokes for digits 1 to 9: 9 keystrokes.\n\n2. Number of keystrokes needed to type each two-digit number from 10 to 99:\n   - Each two-digit number consists of two keystrokes for the tens digit and one keystroke for the units digit.\n   - There are 9 different tens digits (1 to 9) and 10 different units digits (0 to 9).\n   - Total keystrokes for two-digit numbers: 9 * 2 + 10 = 28 keystrokes.\n\n3. Number of keystrokes needed to type each three-digit number from 100 to 500:\n   - Each three-digit number consists of three keystrokes for the hundreds digit, one keystroke for the tens digit, and one keystroke for the units digit.\n   - There are 4 different hundreds digits (1 to 5), 10 different tens digits (0 to 9), and 10 different units digits (0 to 9).\n   - Total keystrokes for three-digit numbers: 4 * 3 + 10 * 2 + 10 = 42 keystrokes.\n\n4. Sum up the total number of keystrokes from the above subproblems:\n   - Total keystrokes = Keystrokes for digits 1 to 9 + Keystrokes for two-digit numbers + Keystrokes for three-digit numbers\n   - Total keystrokes = 9 + 28 + 42 = 79 keystrokes.\n\nTherefore, it would take 79 keystrokes to type the numbers from 1 to 500."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:05:28.261319Z",
     "start_time": "2025-05-20T07:05:28.257411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "pipeline = prompt_template | llm"
   ],
   "id": "b3d50c856290f4ab",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:05:32.061822Z",
     "start_time": "2025-05-20T07:05:29.583740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(result))"
   ],
   "id": "791482ff5b89fddf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "To type the numbers from 1 to 500, we need to consider the number of keystrokes required for each number. \n\n- For numbers 1 to 9, it takes 1 keystroke each.\n- For numbers 10 to 99, it takes 2 keystrokes each.\n- For numbers 100 to 500, it takes 3 keystrokes each.\n\nSo, to calculate the total number of keystrokes needed to type the numbers from 1 to 500:\n\nFor numbers 1 to 9: 9 numbers * 1 keystroke = 9 keystrokes\nFor numbers 10 to 99: 90 numbers * 2 keystrokes = 180 keystrokes\nFor numbers 100 to 500: 401 numbers * 3 keystrokes = 1203 keystrokes\n\nAdding these together:\n9 + 180 + 1203 = 1392\n\nTherefore, it would take 1392 keystrokes to type the numbers from 1 to 500."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chat Memory",
   "id": "1b7c11aebd5d7264"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ConversationBufferMemory Deprecated",
   "id": "301237a3d44abd76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:12:28.096208Z",
     "start_time": "2025-05-20T07:12:27.346442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ],
   "id": "426b1b1e6e9aaee2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cg08900\\AppData\\Local\\Temp\\ipykernel_26796\\262359508.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:13:57.386633Z",
     "start_time": "2025-05-20T07:13:57.381838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my name is James\"},  # user message\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
    "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
    "    {\"output\": \"Very cool!\"}  # AI response\n",
    ")"
   ],
   "id": "46f0539831f27617",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:14:05.964916Z",
     "start_time": "2025-05-20T07:14:05.959933Z"
    }
   },
   "cell_type": "code",
   "source": "memory.load_memory_variables({})",
   "id": "8617c762c764ef67",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:14:59.710012Z",
     "start_time": "2025-05-20T07:14:59.701969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ],
   "id": "cc2e93621c9b854e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:15:28.933743Z",
     "start_time": "2025-05-20T07:15:28.842204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "b7145586b2f71c58",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cg08900\\AppData\\Local\\Temp\\ipykernel_26796\\3112564966.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:16:14.667274Z",
     "start_time": "2025-05-20T07:16:14.168712Z"
    }
   },
   "cell_type": "code",
   "source": "chain.invoke({\"input\": \"what is my name again?\"})",
   "id": "f2f0fbd77515c934",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Your name is James.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ConversationBufferMemory Current LangChain",
   "id": "dfea1804df78d97a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:18:04.170081Z",
     "start_time": "2025-05-20T07:18:04.163054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ],
   "id": "7074635008e50df5",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:18:05.645287Z",
     "start_time": "2025-05-20T07:18:05.641490Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline = prompt_template | llm",
   "id": "4d809760fc412b36",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:19:57.402829Z",
     "start_time": "2025-05-20T07:19:57.397973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ],
   "id": "fbc4f8648bcc2d2e",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:20:06.297702Z",
     "start_time": "2025-05-20T07:20:06.260598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ],
   "id": "48e8a47841c71131",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:21:42.161071Z",
     "start_time": "2025-05-20T07:20:29.774367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ],
   "id": "c50b5fadf0e7056d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZBZBghenyPMQViSX4keKoaaj2TWZ', 'finish_reason': 'stop', 'logprobs': None}, id='run-2974dce1-9831-47fc-a2ef-5764a7228e62-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:24:49.302110Z",
     "start_time": "2025-05-20T07:24:48.868348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ],
   "id": "66f77a36d3f77ff6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 50, 'total_tokens': 55, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZBdBdkGqVXa46GDxoScERGqd4bfB', 'finish_reason': 'stop', 'logprobs': None}, id='run-0c6d9beb-5b6d-47c6-a9fc-88b66de08ed6-0', usage_metadata={'input_tokens': 50, 'output_tokens': 5, 'total_tokens': 55, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ConversationBufferWindowMemory Deprecated",
   "id": "4f1242a1f1222a63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:27:01.769508Z",
     "start_time": "2025-05-20T07:27:01.760660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
   ],
   "id": "700290ef8672f18d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cg08900\\AppData\\Local\\Temp\\ipykernel_26796\\2975042918.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:27:14.272727Z",
     "start_time": "2025-05-20T07:27:14.262938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ],
   "id": "59ad945775d6cb24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:27:32.756096Z",
     "start_time": "2025-05-20T07:27:32.752313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "f701ed3ce17205a0",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:27:39.810811Z",
     "start_time": "2025-05-20T07:27:39.271742Z"
    }
   },
   "cell_type": "code",
   "source": "chain.invoke({\"input\": \"what is my name again?\"})\n",
   "id": "24e71e8babf34f99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: what is my name again?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"I'm sorry, I do not have access to your personal information such as your name.\"}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ConversationBufferWindowMemory Current",
   "id": "a0baed5e4d91482b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:28:55.501657Z",
     "start_time": "2025-05-20T07:28:55.481162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ],
   "id": "ac5b16880f4833e",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:32:02.317500Z",
     "start_time": "2025-05-20T07:32:02.311819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ],
   "id": "a5fad01fdbcc46e5",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:32:34.284958Z",
     "start_time": "2025-05-20T07:32:34.279613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ],
   "id": "67e602a4f19dc24f",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:32:56.070098Z",
     "start_time": "2025-05-20T07:32:55.272502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ],
   "id": "e1b0b3696f470cff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n",
      "Initializing BufferWindowMessageHistory with k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello James! It's nice to meet you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 26, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZBl1cPgmYiMoIUes7ZE3YPo2CYTE', 'finish_reason': 'stop', 'logprobs': None}, id='run-f46b9888-87d5-488f-a289-484c53632560-0', usage_metadata={'input_tokens': 26, 'output_tokens': 17, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:33:19.568090Z",
     "start_time": "2025-05-20T07:33:19.560650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history\n",
    "\n",
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages"
   ],
   "id": "688368eda8b7c169",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:34:05.795179Z",
     "start_time": "2025-05-20T07:34:05.326122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ],
   "id": "8b4a52de5acb5c75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Zeta.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 79, 'total_tokens': 85, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZBm9hhAejIufZ5tGMoaABR127T0V', 'finish_reason': 'stop', 'logprobs': None}, id='run-a22aee25-8b82-4d5c-94f0-cfc512d0ea9c-0', usage_metadata={'input_tokens': 79, 'output_tokens': 6, 'total_tokens': 85, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:34:22.101365Z",
     "start_time": "2025-05-20T07:34:21.629352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ],
   "id": "45d2999c97546b47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n",
      "Initializing BufferWindowMessageHistory with k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello James! It's nice to meet you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 26, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZBmP4pSdkmuOWAQoHnadjBDpaUAe', 'finish_reason': 'stop', 'logprobs': None}, id='run-fed5bdea-3154-45f0-9f99-8fd7f08bace1-0', usage_metadata={'input_tokens': 26, 'output_tokens': 17, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:34:30.188240Z",
     "start_time": "2025-05-20T07:34:30.179823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k14\"].messages"
   ],
   "id": "4beaaecfdbbba238",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello James! It's nice to meet you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 26, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZBmP4pSdkmuOWAQoHnadjBDpaUAe', 'finish_reason': 'stop', 'logprobs': None}, id='run-fed5bdea-3154-45f0-9f99-8fd7f08bace1-0', usage_metadata={'input_tokens': 26, 'output_tokens': 17, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:34:54.130880Z",
     "start_time": "2025-05-20T07:34:53.751074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ],
   "id": "5f9bcb7fc4c6dabe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 169, 'total_tokens': 174, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZBmvuTtorrfzpz3LitUySj7pGYvL', 'finish_reason': 'stop', 'logprobs': None}, id='run-71c8875c-d966-4856-9152-ab3507cfb918-0', usage_metadata={'input_tokens': 169, 'output_tokens': 5, 'total_tokens': 174, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T07:35:10.726870Z",
     "start_time": "2025-05-20T07:35:10.721468Z"
    }
   },
   "cell_type": "code",
   "source": "chat_map[\"id_k14\"].messages",
   "id": "c29009470558816e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello James! It's nice to meet you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 26, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZBmP4pSdkmuOWAQoHnadjBDpaUAe', 'finish_reason': 'stop', 'logprobs': None}, id='run-fed5bdea-3154-45f0-9f99-8fd7f08bace1-0', usage_metadata={'input_tokens': 26, 'output_tokens': 17, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is James.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 169, 'total_tokens': 174, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BZBmvuTtorrfzpz3LitUySj7pGYvL', 'finish_reason': 'stop', 'logprobs': None}, id='run-71c8875c-d966-4856-9152-ab3507cfb918-0', usage_metadata={'input_tokens': 169, 'output_tokens': 5, 'total_tokens': 174, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
